# ğŸ“¦ Vendor Performance and Prediction Analysis

This end-to-end data analytics project investigates vendor sales, purchases, and inventory performance using SQL, Python, and Power BI. It aims to provide actionable insights to the business by solving five core business problems based on real-world transactional data. The project also includes predictive modeling using machine learning for deeper decision-making.

---

## ğŸš€ Problem Statement

Effective inventory and sales management are critical for optimizing profitability in the retail and wholesale industry. Companies need to ensure that they are not incurring losses due to inefficient pricing, poor inventory turnover or vendor dependency. The goal of this analysis is to:

- Identify underperforming brands that require promotional or pricing adjustments.
- Determine top vendors contributing to sales and gross profit.
- Analyze the impact of bulk purchasing on unit cost.
- Assess inventory turnover to reduce holding costs and improve efficiency.
- Investigate profitability variance between high- and low-performing vendors.

---

## ğŸ“ Project Structure

```plaintext
Vendor_Performance_Analysis/
â”‚
â”œâ”€â”€ Raw_Datasets/                      # Original raw datasets (hosted on Drive ğŸ”—)
â”œâ”€â”€ All_CSVs/                          # Processed summary CSVs (from Python EDA)
â”‚
â”œâ”€â”€ db_ingestion.ipynb                 # Python code to ingest CSVs into PostgreSQL
â”œâ”€â”€ sql_eda.ipynb                      # SQL queries for initial exploration
â”œâ”€â”€ python_eda.ipynb                   # Visual and statistical analysis in Python
â”œâ”€â”€ ML_Predictions.ipynb               # Predictive models for vendor performance
â”‚
â”œâ”€â”€ VendorPerformance_Dashboard.pbix   # Interactive Power BI dashboard
â”œâ”€â”€ Project_Summary.docx               # Implementation notes and strategy
â””â”€â”€ README.md                          # ğŸ“ You're here!
```

## ğŸ§ª Step-by-Step Pipeline

### 1ï¸âƒ£ Data Ingestion

- All raw CSV files were loaded using **Python** into a **PostgreSQL** database.
- Due to dataset sizes (up to **1 crore rows**), large files were ingested in **100,000-row chunks** using a custom function.
- Data was stored in normalized tables like: `purchases`, `sales`, `freight`, `purchase_prices`, etc.

---

### 2ï¸âƒ£ SQL Analysis & Aggregation

- Multiple **joins and aggregations** were performed to summarize vendor performance.
- A final summary table called `vendor_sales_summary` was created using **CTEs and JOINS**.
- This summary table was stored back into the database to avoid redundant computations.

---

### 3ï¸âƒ£ Python EDA

- **Outlier detection** using boxplots and histograms.
- **Feature exploration** through correlation heatmaps.
- **Data cleaning and derived features**, including:
  - `StockTurnover`
  - `UnitPurchasePrice`
  - `UnsoldInventoryValue`
